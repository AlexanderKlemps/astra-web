{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75d5434-b33c-4377-9681-6cbdeff3e578",
   "metadata": {},
   "source": [
    "# Concurrent requests to ASTRA web API\n",
    "Author: Alexander Klemps, alexander.klemps@tuhh.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ac3b2-222f-4a0e-b599-264659df8958",
   "metadata": {},
   "source": [
    "Since the aim of the ASTRA API project is to sample simulation data at an amount up to an order of at least $10^4$, it is vital to investigate an approach of concurrent requests and processing to speed up the sampling.\n",
    "\n",
    "There actually already exist concurrent versions of the ASTRA simulation code itself. However, these approaches are based on frameworks like OpenMPI, which are kind of hard to set up and not well documented.\n",
    "Since the API interface fully relies on Docker, a fairly simple approach is given by just creating $n\\in\\mathbb{N}$ of the replicas, each of which can be considered a worker running a simulation.\n",
    "By the help of a simple nginx load balancer, the plan is then to distribute the incoming requests equally to the created worker on the server. \n",
    "\n",
    "So far so good, let's see how that works. First, let's set up the necessary code to generate particles and run a simulation. Refer to the other [notebook](./request_test.ipynb) to get more detailed information on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abaa1de3-04f6-4511-9d76-ef5b3d3be95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, time, asyncio, aiohttp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a84b3-06e7-4ff5-a12d-c5c1b77a0b75",
   "metadata": {},
   "source": [
    "Since we want to carry out an experiment close to the conditions that will be met during a real sampling process, we run the API on a remote server this time, whose DNS is given below (REQUEST_URL).\n",
    "\n",
    "Also it is worth to mention that there are 10 running instances of the ASTRA WebAPI running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c13392c-0597-46b8-ae4f-622ad5deaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request URL pointing to running service on remote server black.dsf.tuhh.de\n",
    "# TUHH VPN required\n",
    "REQUEST_URL = \"http://black/astra/\" \n",
    "# headers include API key secret for authentication\n",
    "REQUEST_HEADERS = {'Content-Type': 'application/json', 'x-api-key': os.getenv(\"ASTRA_API_KEY\")}\n",
    "\n",
    "# simulation data\n",
    "CAVITY_DATA = pd.read_csv(\"./data/cavity_E_field.dat\", names=['z', 'v'], sep=\" \").to_dict('list')\n",
    "SOLENOID_DATA = pd.read_csv(\"./data/solenoid_B_field.dat\", names=['z', 'v'], sep=\" \").to_dict('list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5387604-1c1d-4113-b61d-370e6365936c",
   "metadata": {},
   "source": [
    "Now here comes something new compared to the former notebook. We are not just aiming to run the simulations in parallel on the remote server, but also to receive the results concurrently once the computations are done. This can be realized by asynchronous functions, triggering coroutines.\n",
    "Each of these runs within a session, and the results are gathered once all the workers returned their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3273f433-be6d-4770-ac0a-519ed9850dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _request(endpoint, data, session):\n",
    "    url = REQUEST_URL + endpoint\n",
    "    try:\n",
    "        async with session.post(url=url, headers=REQUEST_HEADERS, json=data) as response:\n",
    "            return await response.json()\n",
    "    except:\n",
    "        print(\"Unable to get url {} due to {}.\".format(url, e.__class__))\n",
    "\n",
    "async def request(endpoint, data):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        return (await asyncio.gather(_request(endpoint, data, session)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43689e98-19f3-49a4-b6b1-2be4a7700d06",
   "metadata": {},
   "source": [
    "Let's generate some particle distribution and set the simulation parameters as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3681d916-b6f0-407b-bfd3-40e46a5a4595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1708083398.789143'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_parameters = { \n",
    "    \"particle_count\": 10000, \n",
    "    \"time_spread\": True,\n",
    "    \"dist_z\": \"flattop\",\n",
    "    \"flattop_time_length\": 3.0*1e-3, # 3ps\n",
    "    \"flattop_rise_time\": 0.2*1e-3, # 3ps\n",
    "    \"rms_bunch_size_x\":0.34E0,\n",
    "    \"rms_bunch_size_y\": 0.34E0,\n",
    "    \"reference_kinetic_energy\": 0.55*1e-6,\n",
    "    \"gaussian_cutoff_x\": 2.0,\n",
    "    \"gaussian_cutoff_y\": 2.0,\n",
    "    \"flattop_z_length\": 3.0,\n",
    "    \"flattop_rise_z\": 0.02,\n",
    "    \"rms_energy_spread\": 1.0019*1e-06,\n",
    "    \"x_emittance\": 0.1473,\n",
    "    \"y_emittance\": 0.1473\n",
    "}\n",
    "\n",
    "response = await request(\"generate\", generator_parameters)\n",
    "distribution_timestamp = response['timestamp']\n",
    "distribution_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa647a5-3700-4ba1-b51f-ccfa35d1f14b",
   "metadata": {},
   "source": [
    "That worked! So for just one sample, our new code performs exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c490e55d-d42c-41eb-8e03-2c13d72c031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_parameters = { \n",
    "    'run_specs': {\n",
    "        'particle_file_name': distribution_timestamp,\n",
    "        'rms_laser_spot_size': 0.2,\n",
    "        'auto_phase': True,\n",
    "        'rms_emission_time': 0.000040, # 40fs\n",
    "    },\n",
    "    'output_specs': {\n",
    "        'z_stop': 1.0,\n",
    "    },\n",
    "    'space_charge': {\n",
    "        'use_space_charge': True,\n",
    "        'z_trans': 0.05,\n",
    "    },\n",
    "    'cavities': [\n",
    "        {\n",
    "            'field_table': CAVITY_DATA,\n",
    "            'max_field_strength': 57.5\n",
    "        },\n",
    "    ],\n",
    "    'solenoids': [\n",
    "        {\n",
    "            'field_table': SOLENOID_DATA, \n",
    "            'max_field_strength': 0.2\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb780b-f2e5-4b00-823c-2839a45ef340",
   "metadata": {},
   "source": [
    "So now about sending more and heavier processes in parallel, such as simulation runs? Let's experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad46a904-1c3b-4f70-91e3-6f7504014bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(endpoint, data, n=3):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        results = await asyncio.gather(*(_request(endpoint, data, session) for i in range(n)))\n",
    "    print(f\"Finalized all requests, N = {n}.\".format(len(results)))\n",
    "    return results\n",
    "\n",
    "async def simulation_experiment(data, N):\n",
    "    start = time.time()\n",
    "    results = await main(\"simulate\", data, n=N)\n",
    "    print(f\"Time required: {time.time() - start} seconds\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42f63a-2882-4799-974f-88d01725154d",
   "metadata": {},
   "source": [
    "We start with one simulation again and measure how long it takes to receive the results back from the remote server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2558034-544c-4b1a-bcc4-b1d13897476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalized all requests, N = 1.\n",
      "Time required: 30.815760135650635 seconds\n"
     ]
    }
   ],
   "source": [
    "results_exp_1 = await simulation_experiment(simulation_parameters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e312e3d-85cc-4bc4-996d-50884b971b58",
   "metadata": {},
   "source": [
    "All in all we need 30 seconds for one simulation to run from start to end and the results to be sent back. We can do better than this! Let's try 10 simulations! Remember that we have 10 workers waiting on the remote machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df516404-6ed2-44ce-9c69-2ecd04d0c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalized all requests, N = 10.\n",
      "Time required: 33.29476451873779 seconds\n"
     ]
    }
   ],
   "source": [
    "results_exp_2 = await simulation_experiment(simulation_parameters, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc47a51-d480-41cf-ac64-670e1d6f8d92",
   "metadata": {},
   "source": [
    "Wow, we almost needed only the same time as for one simulation to run through. It's fair to say though that we were expecting 10 workers to work 10 times faster than a single one. However, having achieved a <b>perfect speedup</b> is quite satisfying.\n",
    "\n",
    "Just to be sure, let's finally check whether we really received 10 results from our workers. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3875a35-f1c9-4547-b171-a264003995b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_exp_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
